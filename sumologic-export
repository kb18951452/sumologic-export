#!/usr/bin/env python3
"""
sumologic-export
~~~~~~~~~~~~~~~~

Export your Sumologic logs easily and quickly.

Usage:
    sumologic-export --configure
    sumologic-export
    sumologic-export <start> <stop> <scope>
    sumologic-export (-h | --help)
    sumologic-export (-v | --version)

Written by Randall Degges (http://www.rdegges.com)
Updated by Caleb Fogleman (https://github.com/cafogleman)
"""

from __future__ import print_function

from datetime import datetime, timedelta
from json import dumps, loads
from os import chmod, mkdir, walk, remove
from os.path import exists, expanduser
from subprocess import call
from time import sleep
from queue import Queue
from threading import Thread
from gzip import compress
from argparse import ArgumentParser

from requests import get, post


##### GLOBALS
VERSION = '0.0.3'
CONFIG_FILE = expanduser('~/.sumo')


# Pretty print datetime objects.
prettify = lambda x: x.strftime('%Y-%m-%dT%H:%M:%S')


class Exporter(object):
    """Abstraction for exporting Sumologic logs."""

    # Partition name to query (or *)
    SCOPE = '*'

    # Default time increment to move forward by.
    INCREMENT = timedelta(minutes=1440)

    # Default timerange to use if no dates are specified.
    DEFAULT_TIMERANGE = timedelta(days=30)

    # Sumologic API constants.
    SUMOLOGIC_URL = 'https://api.us2.sumologic.com/api/v1/search/jobs'
    SUMOLOGIC_HEADERS = {
        'content-type': 'application/json',
        'accept': 'application/json',
    }

    # Amount of time to wait for API response.
    TIMEOUT = 30

    # Sumologic timezone to specify.
    TIMEZONE = 'UTC'

    # Amount of time to pause before requesting Sumologic logs.  60 seconds
    # seems to be a good amount of time.
    SLEEP_SECONDS = 5

    # The amount of logs to download from Sumologic per page.  The higher this
    # is, the more memory is used, but the faster the exports are.
    MESSAGES_PER_PAGE = 10000

    # Number of threads to create
    THREADS = 10
    SAVE_THREADS = 5

    # AWS credential profile for uploading to S3
    PROFILE = 'default'

    def __init__(self):
        """
        Initialize this exporter.

        This includes:

        - Loading credentials.
        - Prepping the environment.
        - Setting up class variables.
        """
        if not exists(CONFIG_FILE):
            print('No credentials found! Run sumologic-export --configure')
            raise SystemExit()

        if not exists('exports'):
            mkdir('exports')

        with open(CONFIG_FILE, 'rb') as cfg:
            creds = loads(cfg.read())
            self.credentials = (creds['access_id'], creds['access_key'])

        self.cookies = None

    def init_dates(self, start, stop):
        """
        Validate and initialize the date inputs we get from the user.

        We'll:

        - Ensure the dates are valid.
        - Perform cleanup.
        - If no dates are specified, we'll set defaults.
        """
        if start:
            try:
                self.start = datetime.strptime(start, '%Y-%m-%d').replace(
                    hour=0,
                    minute=0,
                    second=0,
                    microsecond=0
                )
            except:
                print('Invalid date format. Format must be YYYY-MM-DD.')
                raise SystemExit(1)

            if self.start > datetime.now():
                print('Start date must be in the past!')
                raise SystemExit(1)
        else:
            self.start = (datetime.now() - self.DEFAULT_TIMERANGE).replace(
                hour=0,
                minute=0,
                second=0,
                microsecond=0
            )

        if stop:
            try:
                self.stop = datetime.strptime(stop, '%Y-%m-%d').replace(
                    hour=0,
                    minute=0,
                    second=0,
                    microsecond=0
                )
            except:
                print('Invalid date format. Format must be YYYY-MM-DD.')
                raise SystemExit(1)

            if self.stop > datetime.now():
                print('Stop date must be in the past!')
                raise SystemExit(1)
        else:
            self.stop = datetime.now().replace(
                hour=0,
                minute=0,
                second=0,
                microsecond=0
            )

    def export(self, start, stop, bucket, scope, output):
        """
        Export all Sumologic logs from start to stop.

        All logs will be downloaded one day at a time, and put into a local
        folder named 'exports'.

        :param str scope: The Query for results set
        :param str start: The datetime at which to start downloading logs.
        :param str stop: The datetime at which to stop downloading logs.
        :param str bucket: S3 bucket name for uploading to AWS S3
        """
        # Validate / cleanup the date inputs.
        self.init_dates(start, stop)

        self.SCOPE = scope
        self.OUTPUT = output

        print(f'Exporting all logs from: {prettify(self.start)} to '
              + f'{prettify(self.stop)}... This may take a while.\n')

        print('Exporting Logs')
        print('--------------')

        date = self.start
        q = Queue()
        r = Queue()
        while date < self.stop:

            # Queue up the parts of the job
            job = (date, date + self.INCREMENT)
            q.put(job)
            date += self.INCREMENT

        for i in range(self.THREADS):
            t = Thread(target=self.threaded_download, args=(q, r))
            t.daemon = True
            t.start()

        for j in range(self.SAVE_THREADS):
            if bucket:
                h = Thread(target=self.upload_to_s3, args=(r, bucket))
                h.daemon = True
                h.start()
            else:
                h = Thread(target=self.write_to_file, args=(r,))
                h.daemon = True
                h.start()

        q.join()
        r.join()
        print('\nFinished downloading logs!')

    def upload_to_s3(self, r, bucket):
        """
        Upload gzipped data to AWS S3 bucket.

        :param Queue r: queue of save jobs to be completed
        :param string bucket: S3 bucket name for uploading to AWS S3
        """

        from boto3 import Session
        session = Session(profile_name=self.PROFILE)
        s3 = session.resource('s3')
        bucket = s3.Bucket(bucket)
        while True:
            if r.empty():
                sleep(5)
                continue
            path, data = r.get()
            print(f'uploading {path}')
            bucket.put_object(
                Key=f'{path}',
                Body=data
            )
            r.task_done()

    def write_to_file(self, r):
        """
        Write gzipped data to a local file.

        :param Queue r: queue of save jobs to be completed
        """
        while True:
            if r.empty():
                sleep(5)
                continue
            path, data = r.get()
            with open(path, 'w') as fout:
                print(f'writing {path}')
                for datum in data:
                    fout.write(datum)
            r.task_done()

    def threaded_download(self, q, r):
        """
        Function for workers to start and retrieve jobs, gzip logs, and queue
        them for save, whether local or to cloud, asynchronously.

        :param Queue q: queue of download jobs to be run
        :param Queue r: queue of save jobs to be completed
        """
        while not q.empty():
            job = q.get()
            date = job[0]
            # Schedule the Sumologic job.
            job_url = self.create_job(job[0], job[1])
            print('-', prettify(date))

            # Pause to allow Sumologic to process this job.
            sleep(self.SLEEP_SECONDS)

            # Figure out how many logs there are for the given date.
            total_logs = self.get_count(job_url)

            # If there are logs to be downloaded, let's do it.
            if total_logs:
                print(f' - Downloading {total_logs} logs.')

                logs = []
                for log in self.get_logs(job_url, total_logs):
                    logs.append(log['_raw'].replace("\n","").replace("\t","")+"\n")

                # GZIP logs and queue them for save (local or cloud)
                r.put(
                    (

                        f'exports/{self.OUTPUT}-{prettify(date)}.log',
                        logs
                    )
                )

            else:
                print(' - No logs found.')
            q.task_done()

    def create_job(self, start, stop):
        """
        Request all Sumologic logs for the specified date range.

        :param datetime start: The date to start.
        :param datetime stop: The date to stop.

        :rtype: string
        :returns: The URL of the job.
        """
        while True:
            try:
                resp = post(
                    self.SUMOLOGIC_URL,
                    auth=self.credentials,
                    headers=self.SUMOLOGIC_HEADERS,
                    timeout=self.TIMEOUT,
                    data=dumps({
                        'query': self.SCOPE +
                                 ' | fields _messagetime, _sourceName,' +
                                 ' _sourceCategory, _sourceHost, _raw',
                        'from': start.isoformat(),
                        'to': stop.isoformat(),
                        'timeZone': self.TIMEZONE,
                    }),
                    cookies=self.cookies,
                )
                if resp.cookies:
                    self.cookies = resp.cookies

                if resp.status_code == 202:
                    return '%s/%s' % (self.SUMOLOGIC_URL, resp.json()['id'])

                raise
            except:
                sleep(1)

    def get_count(self, job_url):
        """
        Given a Sumologic job URL, figure out how many logs exist.

        :param str job_url: The job URL.

        :rtype: int
        :returns: The amount of logs found in the specified job results.
        """
        while True:
            try:
                resp = get(
                    job_url,
                    auth=self.credentials,
                    headers=self.SUMOLOGIC_HEADERS,
                    timeout=self.TIMEOUT,
                    cookies=self.cookies
                )
                if resp.cookies:
                    self.cookies = resp.cookies
                if resp.status_code == 200:
                    json = resp.json()
                    if json['state'] == 'DONE GATHERING RESULTS':
                        return json['messageCount']

                raise
            except:
                sleep(1)

    def get_logs(self, job_url, count):
        """
        Iterate through all Sumologic logs for the given job.

        :param str job_url: The job URL.
        :param int count: The number of logs to retrieve.

        :rtype: generator
        :returns: A generator which returns a single JSON log until all logs
            have been retrieved.
        """
        for page in range(0, int(count / self.MESSAGES_PER_PAGE) + 1):
            while True:
                try:
                    resp = get(
                        job_url + '/messages',
                        auth=self.credentials,
                        headers=self.SUMOLOGIC_HEADERS,
                        timeout=self.TIMEOUT,
                        params={
                            'limit': int(self.MESSAGES_PER_PAGE),
                            'offset': int(self.MESSAGES_PER_PAGE * page),
                        },
                        cookies=self.cookies,
                        )
                    if resp.cookies:
                        self.cookies = resp.cookies
                    if resp.status_code == 200:
                        json = resp.json()
                        for log in json['messages']:
                            yield log['map']

                        break

                    raise
                except:
                    sleep(1)


def configure():
    """
    Read in and store the user's Sumologic credentials.

    Credentials will be stored in ~/.sumo
    """
    print('Initializing `sumologic-export`...\n')
    print("To get started, we'll need to get your Sumologic credentials.")

    while True:
        access_id = input('Enter your access id: ').strip()
        access_key = input('Enter your access key: ').strip()
        if not (access_id or access_key):
            print('\nYour Sumologic credentials are needed to continue!\n')
            continue

        print('Your API credentials are stored in:', CONFIG_FILE, '\n')
        print('Run sumologic-export for usage information.')

        with open(CONFIG_FILE, 'w') as cfg:
            cfg.write(dumps({
                'access_id': access_id,
                'access_key': access_key,
            }, indent=2, sort_keys=True))

        # Make the configuration file only accessible to the current user --
        # this makes the credentials a bit more safe.
        chmod(CONFIG_FILE, 0o600)

        break


def main():
    """
    Handle command line options.
    """
    parser = ArgumentParser(
        description="Export logs from Sumologic and write to file (or S3)"
    )
    parser.add_argument(
        '--version', '-v',
        action='store_true',
        help='Print version and exit'
    )
    parser.add_argument(
        '--configure',
        action='store_true',
        help='Configure credentials for use in exporting logs'
    )
    parser.add_argument(
        '--bucket', '-b',
        default='',
        type=str,
        help='Path to S3 bucket for storage'
    )
    parser.add_argument(
        'start',
        type=str,
        help='Log start time',
        nargs='?'
    )
    parser.add_argument(
        'stop',
        type=str,
        help='Log stop time',
        nargs='?'
    )
    parser.add_argument(
        '--scope', '-s',
        default='*',
        type=str,
        help='Query for results set',
        nargs='?'
    )
    parser.add_argument(
        '--output', '-o',
        default='output',
        type=str,
        help='File name for Output',
        nargs='?'
    )
    args = parser.parse_args()

    if args.version:
        print(VERSION)
        raise SystemExit()

    elif args.configure:
        configure()
        raise SystemExit()

    exporter = Exporter()
    exporter.export(args.start, args.stop, bucket=args.bucket, scope=args.scope, output=args.output)


if __name__ == '__main__':
    from sys import exit
    exit(main())
